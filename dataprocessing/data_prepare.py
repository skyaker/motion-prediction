import os
import numpy as np
import torch
import yaml
import time
from l5kit.data import ChunkedDataset, LocalDataManager
from l5kit.dataset import AgentDataset
from l5kit.configs import load_config_data
from l5kit.rasterization import build_rasterizer

# --- Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ ---
with open("../config/config.yaml", "r") as f:
    config = yaml.safe_load(f)

mode = config["hardware"]["mode"]  # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ "weak" Ð¸Ð»Ð¸ "strong"

# --- ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ L5Kit ---
DATA_PATH = config["paths"]["data_path"]
os.environ["L5KIT_DATA_FOLDER"] = DATA_PATH
cfg = load_config_data("../config/config.yaml")  # ÐšÐ¾Ð½Ñ„Ð¸Ð³ L5Kit
dm = LocalDataManager(None)

# --- ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ñ€ÐµÐ¶Ð¸Ð¼Ð° (weak/strong) ---
BATCH_SIZE = config["processing"]["batch_size"][mode]  # 1000 Ð¸Ð»Ð¸ 10 000
SAMPLE_RATE = config["processing"]["sample_rate"][mode]  # 0.005 Ð¸Ð»Ð¸ 0.01
RASTER_SIZE = config["raster_params"]["raster_size"][mode]  # Ð Ð°Ð·Ð¼ÐµÑ€ Ñ€Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
PIXEL_SIZE = tuple(config["raster_params"]["pixel_size"][mode]) 

cfg["raster_params"]["raster_size"] = RASTER_SIZE
cfg["raster_params"]["pixel_size"] = PIXEL_SIZE
cfg["processing"]["batch_size"] = BATCH_SIZE
cfg["processing"]["sample_rate"] = SAMPLE_RATE

# --- Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ ---
zarr_path = dm.require(config["data_loaders"]["sample"]["key"])
dataset = ChunkedDataset(zarr_path).open()

# --- Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ€Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ (ÐºÐ°Ñ€Ñ‚Ñƒ) ---
rasterizer = build_rasterizer(cfg, dm)

# --- Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ `AgentDataset` (Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ X, y) ---
agent_dataset = AgentDataset(cfg, dataset, rasterizer)

LOG_FILE = "data_analyze/dp_log.txt"
os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

def log_message(message):
    """Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ñ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¸ Ð·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÑ‚ Ð² Ð»Ð¾Ð³-Ñ„Ð°Ð¹Ð»"""
    timestamp = time.strftime("[%Y-%m-%d %H:%M:%S]")
    log_entry = f"{timestamp} {message}"
    print(log_entry)
    with open(LOG_FILE, "a") as log:
        log.write(log_entry + "\n")

print(f"âœ… Ð”Ð°Ñ‚Ð°ÑÐµÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½: {len(agent_dataset)} Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð².")

# --- ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ Ð”ÐÐÐÐ«Ð¥ ÐŸÐž Ð‘ÐÐ¢Ð§ÐÐœ ---
output_path = config["paths"]["output_data"]
os.makedirs(output_path, exist_ok=True)

batch_count = 0
X_list, Y_list, Mask_list = [], [], []

start_time = time.time()

for idx, sample in enumerate(agent_dataset):
    # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ sample rate (Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿ÐµÑ€ÐµÐ³Ñ€ÑƒÐ¶Ð°Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ)
    if np.random.rand() > SAMPLE_RATE:
        continue

    X_list.append(sample["image"])  # ÐšÐ°Ñ€Ñ‚Ð° ÑÑ†ÐµÐ½Ñ‹ (Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ)
    Y_list.append(sample["target_positions"])  # Ð‘ÑƒÐ´ÑƒÑ‰Ð¸Ðµ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ñ‹ (Ð²Ñ‹Ñ…Ð¾Ð´)
    Mask_list.append(sample["target_availabilities"])  # ÐœÐ°ÑÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚Ð¸

    print(idx)

    if idx % 1000 == 0:
        elapsed_time = time.time() - start_time
        log_message(f"ðŸŸ¢ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {idx} Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²... Ð’Ñ€ÐµÐ¼Ñ: {elapsed_time:.2f} ÑÐµÐº")

    # ÐšÐ¾Ð³Ð´Ð° Ð½Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼ BATCH_SIZE Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð², ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¸Ñ…
    if len(X_list) >= BATCH_SIZE:
        batch_count += 1
        data_to_save = {
            "images": np.array(X_list, dtype=np.float32),
            "target_positions": np.array(Y_list, dtype=np.float32),
            "target_availabilities": np.array(Mask_list, dtype=np.float32)
        }
        save_path = os.path.join(output_path, f"l5kit_dataset_part{batch_count}.pth")
        torch.save(data_to_save, save_path)

        print(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð±Ð°Ñ‚Ñ‡ {batch_count} ({len(X_list)} Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²) -> {save_path}")

        # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐºÐ¸ Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ Ð¿Ð°Ñ€Ñ‚Ð¸Ð¸
        X_list, Y_list, Mask_list = [], [], []

# Ð•ÑÐ»Ð¸ Ð¾ÑÑ‚Ð°Ð»Ð¸ÑÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ñ„Ð°Ð¹Ð»
if X_list:
    batch_count += 1
    data_to_save = {
        "images": np.array(X_list, dtype=np.float32),
        "target_positions": np.array(Y_list, dtype=np.float32),
        "target_availabilities": np.array(Mask_list, dtype=np.float32)
    }
    save_path = os.path.join(output_path, f"l5kit_dataset_part{batch_count}.pth")
    torch.save(data_to_save, save_path)
    print(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð±Ð°Ñ‚Ñ‡ {batch_count} ({len(X_list)} Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²) -> {save_path}")

print("ðŸŽ‰ Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹ Ðº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ!")
